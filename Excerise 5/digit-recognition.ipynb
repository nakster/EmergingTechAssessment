{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Image Classification With MNIST Dataset</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.simplicity.be/articles/recognizing-handwritten-digits/img/mnist.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "# When the script is run on the computer you need to download keras with anaconda \n",
    "# copy and paste this line in the terminal \n",
    "# conda install -c conda-forge keras \n",
    "# import warnings for unwanted warnings \n",
    "# These are all the imports that are needed \n",
    "from keras.datasets import mnist # dataset\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "# X_train = training images \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train.shape # this prints out the shape of the training images \n",
    "\n",
    "# These are dimensions of the array [60000,23,28]\n",
    "# print(X_train.shape[0])#number of images \n",
    "# print(X_train.shape[1])#width\n",
    "# print(X_train.shape[2])#height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why reshape ?\n",
    "When the Data is loaded from mnist.load_data() the structure of the data is (60000,28,28) i.e images with 2 dimensions 28 x 28. \n",
    "\n",
    "The Convolution2D layers in Keras work with 4 dimensions (sample/batch, height, width, channels) i.e has 4 dimensions input and output but more importantly, it covers deeper layers of the network, where each example has become a set of feature maps i.e. (nb_samples, nb_features, width, height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping to format which CNN expects (batch, height, width, channels)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\" or \"transformation\"\n",
    "\n",
    "Each value is between 0-255 in the MNIST Dataset images. However, this would produce math range errors, So all value are divided by 255 to get decimal values between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train/=255\n",
    "X_test/=255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "One-hot encoding means writing categorical variables in a one-hot vector format, where the vector is all-zero apart from one element. \n",
    "* For example if we are expecting output of 8 so according to one-hot coding its [0,0,0,0,0,0,0,0,1,0]\n",
    "* The index with a non zero value dictates which image label it is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "#number of classes\n",
    "classes = 10\n",
    "# one-hot encoding\n",
    "# we are expecting output as 8 means value of output variable 8\n",
    "# so according to one-hot coding its [0,0,0,0,0,0,0,0,1,0]\n",
    "y_train = np_utils.to_categorical(y_train, classes)\n",
    "y_test = np_utils.to_categorical(y_test, classes)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "# 32 = convolution filters to use, 5 =rows in each convolution kernel,\n",
    "# and 5 = columns in each convolution kernel\n",
    "# input_shape = (depth, width, height)\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(X_train.shape[1], X_train.shape[2], 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model\n",
    "Now that the model is in place we configure the learning process using .compile()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model \n",
    "To fit the model, all we have to do is declare the batch size and number of epochs to train for, then pass in our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 1556s 26ms/step - loss: 0.2914 - acc: 0.9125 - val_loss: 0.0623 - val_acc: 0.9819\n"
     ]
    }
   ],
   "source": [
    "# fit the model # im only doing 1 epoch because the time it takes to run 10 epochs is 5 to 6 hours \n",
    "# but i have ran it for 10 epochs and saved the data in h5 file \n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=200)\n",
    "# Save the model to use test the pictures for later\n",
    "model.save('models/mnistModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics(Test loss & Test Accuracy): \n",
      "Test loss: 2.309396956253052\n",
      "Test accuracy: 0.0797\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "score  = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Metrics(Test loss & Test Accuracy): \")\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import load_model\n",
    "model = load_model('models/mnistModel.h5')\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make a folder which will store the the downloads\n",
    "path = 'data/'\n",
    "#initialise array\n",
    "ndArray = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the MNIST Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for the first option \n",
    "def DownloadFiles():\n",
    "    # If the file does not exist then make a new file \n",
    "    # This makes sure that a file is made even when it doesnt exist\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    # This will store all the rest of urls that i need to download\n",
    "    # The Test Images /Test Labels\n",
    "    urls = ['http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "    # We can go through a for loop and download the files\n",
    "    for url in urls:\n",
    "        # We can then split each url to just the file name \n",
    "        file = url.split('/')[-1]\n",
    "        # print(file)\n",
    "        \n",
    "        #Now in the for loop check if the file exists \n",
    "        #if the file that im downloading already exists\n",
    "        # Then it will not download it \n",
    "        if os.path.exists(path+file):\n",
    "            print('The File Youre trying to download already exists!', file)\n",
    "        else:\n",
    "            #if the file does not exist the it will download the file\n",
    "            print('The',file, 'Is Downloading')\n",
    "            urllib.request.urlretrieve (url, path+file)\n",
    "    print('Done Downloading')\n",
    "\n",
    "    # This here shows how many files exist in the directory\n",
    "    # It should have 2 different files in the folder \n",
    "    # get a list of all the files in the folder 'data'\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    #the for loop goes through each file and extracts it \n",
    "    for file in files:\n",
    "        #checks if the file ends in .gz \n",
    "        if file.endswith('.gz'):\n",
    "            #this reads the file with gzip \n",
    "            with gzip.open(path+file, 'rb') as In:\n",
    "            #removes the .gz file\n",
    "                with open(path+file.split('.')[0], 'wb') as out:\n",
    "                    #shutil copies the contents from In to out\n",
    "                    shutil.copyfileobj(In, out)\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('.gz'):\n",
    "            os.remove(path+file)   \n",
    "        else:\n",
    "            print('All files have been Removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataset to Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToArray():\n",
    "    # This here shows how many files exist in the directory\n",
    "    # It should have 4 different files in the folder \n",
    "    # get a list of all the files in the folder 'data/data'\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    #go through a loop and add the files to the ndarray\n",
    "    for file in files:\n",
    "        #if the extracted file matches then proceed\n",
    "        if file.endswith('ubyte'):\n",
    "            #print('Reading the file', file)\n",
    "            #open the file if the it ends with ubyte and read \n",
    "            with open (path+file,'rb') as f:\n",
    "                #read the file \n",
    "                data = f.read() \n",
    "                # find out the magic number of the file\n",
    "                magic = int.from_bytes(data[0:4], byteorder='big')\n",
    "                # find out the size of the images \n",
    "                size = int.from_bytes(data[4:8], byteorder='big')\n",
    "                \n",
    "                # this is the size of Test images and labels \n",
    "                if (size==10000):\n",
    "                    #here we will know if the file is a test image/label \n",
    "                    trainOrTest = 'test'\n",
    "                # this is the size for training labels and images \n",
    "                elif (size == 60000):\n",
    "                    #here we will know if the file is a Training image/label \n",
    "                    trainOrTest = 'train'\n",
    "                # This checks the magic number 2051 which is for image files \n",
    "                if (magic == 2051):\n",
    "                    imgOrLAbel = 'images'\n",
    "                    #This gets the nummber of rows \n",
    "                    rows = int.from_bytes(data[8:12], byteorder='big')\n",
    "                    #this gets the number of columns \n",
    "                    cols = int.from_bytes(data[12:16], byteorder='big')\n",
    "                    # read values as ints # start from 16 as pixels being from byte 16\n",
    "                    parsed = np.frombuffer(data,dtype = np.uint8, offset = 16) \n",
    "                    # we will reshape the length, 28 x 28 \n",
    "                    parsed = parsed.reshape(size,rows,cols)  \n",
    "                # this checks the magic number 2049 which is for labels \n",
    "                elif (magic == 2049):\n",
    "                    imgOrLAbel = 'labels'\n",
    "                    # read values as ints\n",
    "                    parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "                    #reshape \n",
    "                    parsed = parsed.reshape(size)\n",
    "                #save each file as a array with their key \n",
    "                ndArray[trainOrTest+'_'+imgOrLAbel] = parsed\n",
    "        else:\n",
    "            print('No File Found')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The t10k-images-idx3-ubyte.gz Is Downloading\n",
      "The t10k-labels-idx1-ubyte.gz Is Downloading\n",
      "Done Downloading\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# DownloadFiles() # uncomment if you want to download the files again\n",
    "saveToArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "[Warnings](https://stackoverflow.com/questions/48340392/futurewarning-conversion-of-the-second-argument-of-issubdtype-from-float-to)<br/>\n",
    "[Reshaping of data?](https://datascience.stackexchange.com/questions/11704/reshaping-of-data-for-deep-learning-using-keras)<br/>\n",
    "[Keras sequential](https://keras.io/models/sequential/)<br/>\n",
    "[kerasC](https://anaconda.org/conda-forge/kerasC)<br/>\n",
    "[tutorial](https://elitedatascience.com/keras-tutorial-deep-learning-in-python)<br/>\n",
    "[reshaping-of-data-for-deep-learning-using-keras](https://datascience.stackexchange.com/questions/11704/reshaping-of-data-for-deep-learning-using-keras)<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
